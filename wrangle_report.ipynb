{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Gathering Data\n",
    "The first dataset I gathered was downloaded directly through the link provided in the classroom and the file was stored as _twitter-archive-enhanced.csv_. The second dataset I gathered was downloaded programmatically from one of Udacity's servers using the Requests library through the URL link provided in the classroom and was stored as _image-predictions.tsv_. The third dataset I gathered was gotten by using the tweet IDs in the WeRateDogs Twitter archive to query the Twitter API for each tweet's JSON data with Python's Tweepy library and storing each tweet's entire set of JSON data in a file called _tweet_json.txt_. After I successfully gathered the three datasets, I proceeded to assessing them by checking for quality issues and tidiness issues in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Assessing Data\n",
    "I assessed the three datasets visually to check for missing values and then assessed them programmatically to check for issues like the data type of the respective columns, total number of entries with null values in the respective columns, unique dog names and dogs with no names in the name column amongst other issues. I also checked for the total number of original tweets, the statistical description of the three (3) datasets as well as duplicated columns in the three(3) datasets. Below are some of the issues that I discovered in the course of assessing the three (3) datasets:-\n",
    "\n",
    "### Quality issues\n",
    "1. Missing records in the three datasets\n",
    "\n",
    "1. Some entries in the three datasets are retweets and as such redundant\n",
    "\n",
    "#### twitter-archive-enhanced table\n",
    "3. The datatype in timestamp column is string instead of datetime\n",
    "\n",
    "4. Incorrect values in rating_numerator (e.g 960, 1776, etc) and rating_denominator (e.g 130, 2, etc) columns\n",
    "\n",
    "5. Some entries in expanded_urls column have two or more url links instead of one url link\n",
    "\n",
    "6. Dogs that had no names were recorded as 'a', 'the', 'an', 'one', 'actually' and 'old' in name column \n",
    "\n",
    "#### image-predictions table\n",
    "7. The datatype in p1, p2 and p3 columns should be in categorical format instead of string\n",
    "\n",
    "8. Some entries have invalid or false predictions (p1_dog, p2_dog and p3_dog columns) of dog breeds\n",
    "\n",
    "### Tidiness issues\n",
    "1. The three datasets I gathered are small pieces or fragments of a master dataset\n",
    "\n",
    "#### twitter-archive-enhanced table\n",
    "2. doggo, floofer, pupper and puppo columns should be combined into one column called dog_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cleaning Data\n",
    "I made copies of the original pieces of the data and then defined the cleaning steps as follows:\n",
    "\n",
    "#### Quality Issues\n",
    "1. Drop rows that are retweets (rows whose retweeted_status_id value are non-null)\n",
    "\n",
    "2. Convert the datatype in timestamp column from string object to datetime\n",
    "\n",
    "3. Drop rows with outliers in rating_numerator and rating_denominator columns\n",
    "\n",
    "4. Create a new column called dog_rating to calculate the ratio of rating_numerator to rating_denominator\n",
    "\n",
    "5. Drop columns with retweets information and other columns that are not useful in our analysis\n",
    "\n",
    "6. Replace dogs that have no names in name column with null values\n",
    "\n",
    "7. Convert p1, p2 and p3 columns from string data type to categorical\n",
    "\n",
    "8. Drop rows with invalid or false predictions of dog breeds\n",
    "\n",
    "#### Tidiness Issues\n",
    "1. Merge the four dog stages colums\n",
    "\n",
    "2. Merge all dataframes\n",
    "\n",
    "I followed the Define-Code-Test approach in the cleaning phase. After defining the cleaning steps, I wrote some lines of codes to clean the data by following the defined steps and I then ran some tests to confirm whether the required changes has been effected. After I successfully cleaned each dataset separately, I then combine them together to form a master dataset and stored the master dataset as a file called twitter_archive_master.csv before analyzing and visualizing the cleaned master dataset to draw insights from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
